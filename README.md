![image](https://github.com/mytechnotalent/MicroGPT/blob/main/MicroGPT.png?raw=true)

## FREE Reverse Engineering Self-Study Course [HERE](https://github.com/mytechnotalent/Reverse-Engineering-Tutorial)

<br>

# MicroGPT

A minimal GPT-style transformer language model implementation from scratch in PyTorch, designed for learning and experimentation.

## ğŸ“š Overview

MicroGPT is a decoder-only transformer language model following the GPT (Generative Pre-trained Transformer) architecture. This implementation prioritizes clarity and educational value while maintaining the core components of modern language models.

### Architecture Components

- **Token Embeddings**: Maps vocabulary indices to dense vectors
- **Positional Embeddings**: Encodes sequence position information
- **Multi-Head Self-Attention**: Allows the model to focus on different parts of the input
- **Feed-Forward Networks**: Processes information within each position
- **Layer Normalization**: Stabilizes training
- **Causal Masking**: Ensures autoregressive generation (no peeking at future tokens)

## ğŸ—‚ï¸ Project Structure

### Core Files

```
microgpt/
â”œâ”€â”€ MicroGPT_Tutorial.pdf       # ğŸ“– Complete GPT tutorial (beautifully formatted PDF)
â”œâ”€â”€ micro_gpt.py                # ğŸ Production-ready Python module with all components
â””â”€â”€ test_micro_gpt.py           # ğŸ§ª Comprehensive test suite (65 tests, 99% coverage)
```

### Supporting Files

```
â”œâ”€â”€ example.py                  # Usage example script
â”œâ”€â”€ convert_tutorial_to_pdf.py  # Script to regenerate PDF from TUTORIAL.md
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ TUTORIAL.md                 # Source markdown for the PDF tutorial
â”œâ”€â”€ REFACTORING_SUMMARY.md      # Test suite refactoring documentation
â”œâ”€â”€ CONSISTENCY_UPDATE.md       # Consistency improvements documentation
â”œâ”€â”€ CONSISTENCY_STANDARDS.md    # Quick reference for code standards
â”œâ”€â”€ htmlcov/                    # Coverage reports (generated)
â””â”€â”€ checkpoints/                # Saved model checkpoints (generated)
```

## ğŸ“ Key Files

### 1. ğŸ“– MicroGPT_Tutorial.pdf
**Complete educational resource explaining GPT from scratch**
- 600+ lines of comprehensive explanations
- Covers tokenization, embeddings, attention, transformers, training, and generation
- Written for high school students and above
- Professional formatting with syntax-highlighted code blocks
- Perfect for learning or teaching

### 2. ğŸ micro_gpt.py
**Production-ready Python implementation**
- Clean, well-documented code with 100% type annotations
- All GPT components: `SelfAttentionHead`, `MultiHeadAttention`, `FeedForward`, `Block`, `MicroGPT`
- Training utilities: `get_batch`, `save_checkpoint`, `_train_step`, `_eval_model`
- Complete docstrings with examples for every function
- ~750 lines of thoroughly documented code

### 3. ğŸ§ª test_micro_gpt.py
**Comprehensive test suite**
- 65 tests covering all functionality
- 99% code coverage
- Tests for initialization, forward passes, attention, generation, training, and edge cases
- Follows AAA (Arrange-Act-Assert) pattern
- Full type annotations and documentation

## ğŸ“¦ Additional Files

- **example.py**: Demonstration script showing how to use micro_gpt.py
- **convert_tutorial_to_pdf.py**: Regenerate the PDF from TUTORIAL.md
- **TUTORIAL.md**: Source markdown file (generates the PDF)
- **requirements.txt**: Python dependencies

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

### Setup

1. **Clone or download the repository**

2. **Create and activate a virtual environment:**

```bash
# Create virtual environment
python -m venv .venv

# Activate on macOS/Linux
source .venv/bin/activate

# Activate on Windows
.venv\Scripts\activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

Required packages:
- `torch` - PyTorch deep learning framework
- `tiktoken` - OpenAI's BPE tokenizer
- `datasets` - Hugging Face datasets library
- `pytest` - Testing framework
- `pytest-cov` - Coverage reporting

## ğŸš€ Quick Start

### Using the Python Module

The `micro_gpt.py` module provides a clean, importable interface:

```python
from micro_gpt import MicroGPT, get_batch, save_checkpoint
import torch

# Create model
model = MicroGPT(
    vocab_size=50257,      # GPT-2 vocabulary size
    embedding_dim=768,     # Model dimension
    block_size=256,        # Context window
    n_heads=12,            # Attention heads
    n_layers=12,           # Transformer blocks
    dropout=0.1            # Dropout rate
)

# Forward pass
x = torch.randint(0, 50257, (4, 256))  # (batch_size, seq_len)
logits, loss = model(x)

# Generate text
context = torch.tensor([[1, 2, 3]])  # Start tokens
generated = model.generate(
    context, 
    max_new_tokens=100, 
    temperature=0.8
)

# Training utilities
data = torch.randint(0, 50257, (100000,))
x_batch, y_batch = get_batch(
    data, 
    block_size=256, 
    batch_size=32, 
    device="cpu"
)
```

### Running the Example

```bash
python example.py
```

This demonstrates:
1. Creating a MicroGPT model
2. Training on synthetic data
3. Evaluating on validation data
4. Generating text autoregressively
5. Saving and loading checkpoints

### Model Classes

All classes are available from `micro_gpt.py`:

```python
from micro_gpt import (
    SelfAttentionHead,
    MultiHeadAttention,
    FeedForward,
    Block,
    MicroGPT
)
```

#### `SelfAttentionHead`
Single causal self-attention head with scaled dot-product attention.

```python
head = SelfAttentionHead(
    embedding_dim=384,
    block_size=256,
    head_size=64,
    dropout=0.1
)
# Forward: (B, T, embedding_dim) -> (B, T, head_size)
```

#### `MultiHeadAttention`
Multiple attention heads running in parallel with projection.

```python
mha = MultiHeadAttention(
    embedding_dim=384,
    block_size=256,
    num_heads=6,
    dropout=0.1
)
# Forward: (B, T, embedding_dim) -> (B, T, embedding_dim)
```

#### `FeedForward`
Two-layer MLP with ReLU activation and 4x expansion.

```python
ff = FeedForward(n_embd=384, dropout=0.1)
# Forward: (B, T, n_embd) -> (B, T, n_embd)
```

#### `Block`
Complete transformer block with attention and feed-forward layers.

```python
block = Block(
    embedding_dim=384,
    block_size=256,
    n_heads=6,
    dropout=0.1
)
# Forward: (B, T, embedding_dim) -> (B, T, embedding_dim)
```

#### `MicroGPT`
Full language model combining all components.

```python
model = MicroGPT(
    vocab_size=50257,       # GPT-2 tokenizer vocab size
    embedding_dim=384,
    block_size=256,
    n_heads=6,
    n_layers=6,
    dropout=0.1
)

# Forward pass (training)
logits, loss = model(input_ids, targets)  # loss computed if targets provided

# Forward pass (inference)
logits, _ = model(input_ids)  # loss is None

# Text generation
output = model.generate(
    idx=context_tokens,
    max_new_tokens=50,
    temperature=0.8  # Controls randomness (0.1-2.0)
)
```

### Utility Functions

Import from `micro_gpt.py`:

```python
from micro_gpt import (
    get_batch,
    save_checkpoint,
    _train_step,
    _eval_model,
    _should_save
)
```

- **`get_batch(data, block_size, batch_size, device)`** - Sample random training batches
- **`save_checkpoint(model, optimizer, step, val_loss, filepath)`** - Save model checkpoints
- **`_train_step(model, optimizer, train_data, block_size, batch_size, device)`** - Single training step
- **`_eval_model(model, data, block_size, batch_size, device, eval_iters)`** - Evaluate on validation set
- **`_should_save(val_loss, best_loss)`** - Check if current loss is best

## ğŸ“– API Documentation

### Complete Class and Function Signatures

Every class and function in `micro_gpt.py` includes comprehensive docstrings with:
- Detailed descriptions
- Args documentation
- Returns documentation
- Usage examples
- Implementation notes

View docstrings in Python:
```python
from micro_gpt import MicroGPT
help(MicroGPT)              # View class documentation
help(MicroGPT.generate)     # View method documentation
```

### Code Quality Standards

Both `micro_gpt.py` and `test_micro_gpt.py` follow strict consistency standards:

âœ… **100% Type Annotation Coverage**
- All functions have complete type hints for parameters and return values
- Uses proper PyTorch types: `nn.Module`, `torch.Tensor`, `torch.optim.Optimizer`

âœ… **Comprehensive Documentation**
- Every function includes Examples section with doctest-style code
- Consistent docstring format across all files
- Returns sections for all functions (including `-> None`)

âœ… **Production Ready**
- IDE-friendly with IntelliSense support
- Type checker compatible (mypy, pylance)
- Doctest validated examples

**Documentation Resources:**
- `CONSISTENCY_UPDATE.md` - Detailed change summary
- `CONSISTENCY_STANDARDS.md` - Quick reference for contributors

**Type Checking:**
```bash
mypy micro_gpt.py           # Verify type annotations
python -m doctest micro_gpt.py -v  # Test all examples
```

## ğŸ’¡ Example Usage

A complete example script is provided in `example.py`:

```bash
python example.py
```

This demonstrates:
1. Creating a MicroGPT model
2. Training on synthetic data
3. Evaluating on validation data
4. Generating text autoregressively
5. Saving and loading checkpoints

**Example Code:**

```python
from micro_gpt import MicroGPT, get_batch, save_checkpoint
import torch
import torch.optim as optim

# 1. Create model
model = MicroGPT(
    vocab_size=256,
    embedding_dim=128,
    block_size=64,
    n_heads=4,
    n_layers=3,
    dropout=0.1
)

# 2. Setup training
optimizer = optim.Adam(model.parameters(), lr=3e-4)
train_data = torch.randint(0, 256, (10000,))

# 3. Training step
x, y = get_batch(train_data, block_size=64, batch_size=16, device="cpu")
logits, loss = model(x, y)
loss.backward()
optimizer.step()

# 4. Generate text
context = torch.randint(0, 256, (1, 10))
generated = model.generate(context, max_new_tokens=20, temperature=0.8)

# 5. Save checkpoint
save_checkpoint(model, optimizer, step=100, val_loss=2.5, 
                filepath="checkpoints/model.pt")
```

## ğŸ§ª Testing

The project includes a comprehensive test suite with **65 tests** achieving **99% code coverage**.

### Test Organization

```
test_micro_gpt.py
â”œâ”€â”€ TestSelfAttentionHead (6 tests)
â”‚   â”œâ”€â”€ Initialization
â”‚   â”œâ”€â”€ Forward pass shapes
â”‚   â”œâ”€â”€ Causal masking
â”‚   â”œâ”€â”€ Attention weight normalization
â”‚   â”œâ”€â”€ Different sequence lengths
â”‚   â””â”€â”€ Dropout effects
â”‚
â”œâ”€â”€ TestMultiHeadAttention (4 tests)
â”‚   â”œâ”€â”€ Initialization
â”‚   â”œâ”€â”€ Forward pass shapes
â”‚   â”œâ”€â”€ Different head counts
â”‚   â””â”€â”€ Head size computation
â”‚
â”œâ”€â”€ TestFeedForward (4 tests)
â”‚   â”œâ”€â”€ Initialization
â”‚   â”œâ”€â”€ Forward pass shapes
â”‚   â”œâ”€â”€ Expansion ratio (4x)
â”‚   â””â”€â”€ Different embedding dimensions
â”‚
â”œâ”€â”€ TestBlock (4 tests)
â”‚   â”œâ”€â”€ Initialization
â”‚   â”œâ”€â”€ Forward pass shapes
â”‚   â”œâ”€â”€ Residual connections
â”‚   â””â”€â”€ Layer normalization
â”‚
â”œâ”€â”€ TestMicroGPT (15 tests)
â”‚   â”œâ”€â”€ Initialization
â”‚   â”œâ”€â”€ Forward with/without targets
â”‚   â”œâ”€â”€ Loss computation
â”‚   â”œâ”€â”€ Text generation
â”‚   â”œâ”€â”€ Temperature sampling
â”‚   â”œâ”€â”€ Context window handling
â”‚   â”œâ”€â”€ Token embeddings
â”‚   â”œâ”€â”€ Block application
â”‚   â”œâ”€â”€ Gradient flow
â”‚   â””â”€â”€ Parameter counting
â”‚
â”œâ”€â”€ TestGetBatch (5 tests)
â”œâ”€â”€ TestSaveCheckpoint (3 tests)
â”œâ”€â”€ TestTrainStep (3 tests)
â”œâ”€â”€ TestEvalModel (3 tests)
â”œâ”€â”€ TestShouldSave (4 tests)
â”œâ”€â”€ TestIntegration (5 tests)
â”œâ”€â”€ TestEdgeCases (6 tests)
â””â”€â”€ TestPerformance (3 tests)
```

### Running Tests

#### Run All Tests

```bash
pytest test_micro_gpt.py -v
```

Expected output:
```
====================================================== test session starts =======================================================
collected 65 items

test_micro_gpt.py::TestSelfAttentionHead::test_initialization PASSED                                                      [  1%]
test_micro_gpt.py::TestSelfAttentionHead::test_forward_shape PASSED                                                       [  3%]
...
====================================================== 65 passed in 2.86s ========================================================
```

#### Run Specific Test Class

```bash
# Test only MicroGPT model
pytest test_micro_gpt.py::TestMicroGPT -v

# Test only integration tests
pytest test_micro_gpt.py::TestIntegration -v

# Test only edge cases
pytest test_micro_gpt.py::TestEdgeCases -v
```

#### Run Specific Test

```bash
pytest test_micro_gpt.py::TestMicroGPT::test_forward_with_targets -v
```

#### Run with More Detail

```bash
# Show full test output
pytest test_micro_gpt.py -vv

# Show print statements
pytest test_micro_gpt.py -v -s

# Stop at first failure
pytest test_micro_gpt.py -x
```

### Coverage Reports

#### Generate Coverage Report

```bash
# Terminal + HTML report
pytest test_micro_gpt.py -v --cov=test_micro_gpt --cov-report=html --cov-report=term

# Terminal report only
pytest test_micro_gpt.py --cov=test_micro_gpt --cov-report=term

# HTML report only
pytest test_micro_gpt.py --cov=test_micro_gpt --cov-report=html
```

#### View HTML Coverage Report

```bash
# macOS
open htmlcov/index.html

# Linux
xdg-open htmlcov/index.html

# Windows
start htmlcov/index.html
```

The HTML report provides:
- Line-by-line coverage visualization
- Missing line indicators
- Branch coverage analysis
- Interactive file navigation

#### Coverage Report Output

```
======================================================== tests coverage =========================================================
Name                Stmts   Miss  Cover   Missing
-------------------------------------------------
test_micro_gpt.py     521      1    99%   1018
-------------------------------------------------
TOTAL                 521      1    99%
Coverage HTML written to dir htmlcov
```

### Continuous Testing

For development, use pytest-watch to automatically run tests on file changes:

```bash
# Install pytest-watch
pip install pytest-watch

# Watch for changes and run tests
ptw test_micro_gpt.py -- -v --cov=test_micro_gpt --cov-report=term
```

## ğŸ¯ Key Features Tested

### Model Architecture
- âœ… Proper initialization of all layers
- âœ… Correct tensor shapes throughout forward pass
- âœ… Causal attention masking (no future leakage)
- âœ… Residual connections
- âœ… Layer normalization
- âœ… Dropout application

### Training & Optimization
- âœ… Loss computation accuracy
- âœ… Gradient flow verification
- âœ… Weight updates during training
- âœ… Gradient clipping
- âœ… Training/eval mode switching
- âœ… Loss reduction over training steps

### Text Generation
- âœ… Autoregressive generation
- âœ… Temperature-based sampling
- âœ… Context window management
- âœ… Token-by-token generation
- âœ… Batch generation support

### Checkpoint Management
- âœ… Model state saving
- âœ… Optimizer state saving
- âœ… Checkpoint loading
- âœ… State restoration accuracy

### Edge Cases
- âœ… Single token sequences
- âœ… Maximum sequence length
- âœ… Various batch sizes (1-32)
- âœ… Various sequence lengths
- âœ… Temperature extremes (0.01-2.0)

### Performance
- âœ… Forward pass timing
- âœ… Generation speed
- âœ… Memory efficiency
- âœ… No memory leaks

## ğŸ“Š Model Training

### Training on OpenWebText

The model is trained on the OpenWebText dataset, a recreation of OpenAI's WebText:

```python
# Load dataset
train_data, val_data, vocab_size, tokenizer = load_openwebtext(
    max_tokens=50_000_000
)

# Create model
model = MicroGPT(
    vocab_size=vocab_size,
    embedding_dim=384,
    block_size=256,
    n_heads=6,
    n_layers=6,
    dropout=0.2
)

# Train
best_loss = train_model(model, optimizer, train_data, val_data, config)
```

### Checkpointing

Models are automatically saved when validation loss improves:

```
checkpoints/
â”œâ”€â”€ best_val2.5432.pt
â”œâ”€â”€ best_val2.3456.pt
â””â”€â”€ best_val2.1234.pt  # Best model
```

Each checkpoint contains:
- Model state dict
- Optimizer state dict
- Training step number
- Validation loss

### Text Generation

Generate text from trained model:

```python
prompt = "The future of artificial intelligence is"
generated_text = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_tokens=100,
    device="cuda",  # or "cpu"
    temp=0.8
)
print(generated_text)
```

Temperature controls randomness:
- **Low (0.1-0.5)**: More focused, deterministic
- **Medium (0.6-1.0)**: Balanced creativity
- **High (1.0-2.0)**: More random, creative

## ğŸ”§ Configuration

### Device Selection

The model automatically selects the best available device:

```python
device = (
    "cuda" if torch.cuda.is_available()           # NVIDIA GPU
    else "mps" if torch.backends.mps.is_available()  # Apple Silicon
    else "cpu"                                     # CPU fallback
)
```

### Hyperparameter Tuning

Key hyperparameters to adjust:

| Parameter       | Description     | Typical Range |
| --------------- | --------------- | ------------- |
| `embedding_dim` | Model width     | 128-768       |
| `n_layers`      | Model depth     | 2-12          |
| `n_heads`       | Attention heads | 4-12          |
| `block_size`    | Context window  | 128-2048      |
| `dropout`       | Regularization  | 0.0-0.3       |
| `lr`            | Learning rate   | 1e-4 to 1e-3  |
| `batch_size`    | Batch size      | 32-128        |

### Memory Considerations

Approximate memory usage (training):

| Config              | Parameters | GPU Memory |
| ------------------- | ---------- | ---------- |
| Small (d=128, L=2)  | ~1M        | ~2 GB      |
| Medium (d=384, L=6) | ~10M       | ~8 GB      |
| Large (d=768, L=12) | ~100M      | ~20 GB     |

## ğŸ› Troubleshooting

### Common Issues

#### Out of Memory Error

```python
# Reduce batch size
config["batch_size"] = 32  # or 16, 8

# Reduce model size
config["embedding_dim"] = 256
config["n_layers"] = 4
```

#### Slow Training

```python
# Use mixed precision training (if GPU available)
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
```

#### Tests Failing

```bash
# Check PyTorch installation
python -c "import torch; print(torch.__version__)"

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Run with verbose output
pytest test_micro_gpt.py -vv -s
```

#### Import Errors

```bash
# Ensure virtual environment is activated
source .venv/bin/activate  # macOS/Linux
.venv\Scripts\activate     # Windows

# Verify pytest is installed
pip list | grep pytest
```

## ğŸ“ˆ Results & Benchmarks

### Test Suite Performance

- **Total Tests**: 65
- **Pass Rate**: 100%
- **Code Coverage**: 99%
- **Execution Time**: ~2.86 seconds
- **Platform**: CPU (Apple Silicon M-series)

### Model Performance

Training progress (example):

```
Step     0 | Train: 10.8234 | Val: 10.5432
Step   100 | Train:  6.2341 | Val:  6.1234
Step   200 | Train:  4.5234 | Val:  4.4123
Step   500 | Train:  3.2145 | Val:  3.1234 â† BEST
Step  1000 | Train:  2.8234 | Val:  2.9123
...
```

## ğŸ“š Learning Resources

### Complete Tutorial

MicroGPT includes a comprehensive **600+ line tutorial** ([TUTORIAL.md](TUTORIAL.md)) that explains every component of GPT from scratch:

- Tokenization and embeddings
- Positional encoding
- Self-attention mechanisms
- Multi-head attention
- Feed-forward networks
- Transformer blocks
- Training and generation
- Complete walkthrough with examples

**Perfect for**: Students, educators, and anyone wanting to deeply understand how GPT works!

### PDF Version

Generate a beautifully formatted PDF of the tutorial:

```bash
python convert_tutorial_to_pdf.py
```

This creates `MicroGPT_Tutorial.pdf` with:
- Professional formatting
- Syntax-highlighted code blocks
- Preserved tables and diagrams
- Page numbers and headers
- Print-ready quality

**Requirements**: `markdown`, `weasyprint`, `pygments` (included in requirements.txt)

## ğŸ¤ Contributing

### Running Tests Before Commit

```bash
# Run full test suite
pytest test_micro_gpt.py -v

# Check coverage
pytest test_micro_gpt.py --cov=test_micro_gpt --cov-report=term

# Ensure 99%+ coverage maintained
```

### Adding New Tests

1. Add test methods to appropriate test class
2. Follow naming convention: `test_<feature>_<behavior>`
3. Include docstrings explaining what's being tested
4. Run coverage to ensure new code is tested

Example:

```python
class TestMicroGPT:
    def test_new_feature(self, model):
        """Test that new feature works correctly."""
        # Arrange
        input_data = torch.randint(0, 100, (2, 10))
        
        # Act
        output = model.new_feature(input_data)
        
        # Assert
        assert output.shape == (2, 10, 100)
```

## ğŸ“š References

### Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (Radford et al., 2019)

### Resources
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ‘¤ Author

**Kevin Thomas**
- Email: ket189@pitt.edu

## ğŸ™ Acknowledgments

- OpenAI for GPT architecture and research
- Hugging Face for datasets and tools
- PyTorch team for the deep learning framework
- The open-source ML community

---

## Quick Start Checklist

- [ ] Clone/download repository
- [ ] Create virtual environment: `python -m venv .venv`
- [ ] Activate environment: `source .venv/bin/activate`
- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Run tests: `pytest test_micro_gpt.py -v`
- [ ] Generate coverage: `pytest test_micro_gpt.py --cov=test_micro_gpt --cov-report=html`
- [ ] View coverage: `open htmlcov/index.html`
- [ ] Open notebook: `jupyter notebook MicroGPT.ipynb`
- [ ] Train model and experiment!

---

**Happy Training! ğŸš€**
