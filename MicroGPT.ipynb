{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f86e90",
   "metadata": {},
   "source": [
    "# MicroGPT\n",
    "\n",
    "A minimal GPT implementation from scratch in PyTorch that learns to predict the next word in a sequence using self-attention and transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a9c39",
   "metadata": {},
   "source": [
    "### [dataset](https://www.kaggle.com/datasets/mytechnotalent/mary-had-a-little-lamb)\n",
    "\n",
    "Author: [Kevin Thomas](mailto:ket189@pitt.edu)\n",
    "\n",
    "License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de91986",
   "metadata": {},
   "source": [
    "## Chain Rule of Probability\n",
    "\n",
    "The joint probability of variables $x_1,\\dots,x_n$ can be decomposed as:\n",
    "\n",
    "$$\n",
    "P(x_1,\\dots,x_n) = \\prod_{i=1}^n P\\big(x_i \\mid x_1,\\dots,x_{i-1}\\big).\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\ldots, w_n) = P(w_1)\\times P(w_2\\mid w_1)\\times P(w_3\\mid w_1,w_2)\\times \\cdots \\times P(w_n\\mid w_1,\\ldots,w_{n-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914b289",
   "metadata": {},
   "source": [
    "### 1st Iteration\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{Mary}\n",
    "$$\n",
    "\n",
    "Predict:\n",
    "\n",
    "$$ \n",
    "\\text{Mary had}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79799d32",
   "metadata": {},
   "source": [
    "### 2nd Iteration\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{Mary had}\n",
    "$$\n",
    "\n",
    "Predict:\n",
    "\n",
    "$$ \n",
    "\\text{Mary had a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc032316",
   "metadata": {},
   "source": [
    "### 3rd Iteration\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{Mary had a}\n",
    "$$\n",
    "\n",
    "Predict:\n",
    "\n",
    "$$ \n",
    "\\text{Mary had a little}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c42007",
   "metadata": {},
   "source": [
    "### 4th Iteration w/ Probs\n",
    "\n",
    "$$\n",
    "P(\\text{lamb} \\mid \\text{Mary had a little}) = .8\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{dog} \\mid \\text{Mary had a little}) = .1\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{cat} \\mid \\text{Mary had a little}) = .1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4eaae",
   "metadata": {},
   "source": [
    "## Transformer Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f59fc459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\gpt\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c0310926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8edc1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\"A single causal self-attention head.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_dim : int\n",
    "        Dimensionality of the input embeddings (C).\n",
    "    block_size : int\n",
    "        Maximum sequence length supported (used to build a causal mask).\n",
    "    head_size : int\n",
    "        Dimensionality of keys/queries/values for this head.\n",
    "\n",
    "    Shapes\n",
    "    ------\n",
    "    - Input: (B, T, C)\n",
    "    - Output: (B, T, head_size)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The module registers a lower-triangular mask so it can apply causal\n",
    "      attention (tokens cannot attend to future tokens).\n",
    "    - Scaling uses the key dimensionality for numerical stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, block_size: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute causal self-attention for one head.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, T, C).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, T, head_size) after applying attention to values.\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Scale by sqrt(head_dim) for numerical stability\n",
    "        wei = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)\n",
    "        # Apply causal mask\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Weighted sum of values\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention by concatenating multiple `SelfAttentionHead`s and\n",
    "    projecting back to the model `embedding_dim`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_dim : int\n",
    "        Model embedding dimensionality (C).\n",
    "    block_size : int\n",
    "        Maximum sequence length supported (used to create causal masks in heads).\n",
    "    num_heads : int\n",
    "        Number of attention heads. `embedding_dim` must be divisible by `num_heads`.\n",
    "\n",
    "    Shapes\n",
    "    ------\n",
    "    - Input: (B, T, C)\n",
    "    - Output: (B, T, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, block_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(embedding_dim, block_size, head_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply multi-head attention to input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, T, C).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, T, C).\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple two-layer feed-forward network used inside transformer blocks.\n",
    "\n",
    "    Uses a 4x expansion on the embedding dim by default (following common practice).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_embd : int\n",
    "        Embedding dimensionality (C).\n",
    "\n",
    "    Shapes\n",
    "    ------\n",
    "    - Input: (B, T, C)\n",
    "    - Output: (B, T, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply the MLP element-wise across sequence positions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, T, C).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, T, C).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: multi-head self-attention followed by a feed-forward MLP.\n",
    "\n",
    "    Each sub-layer has a pre-layer-norm + residual connection:\n",
    "\n",
    "        x = x + self_attention(LayerNorm(x))\n",
    "        x = x + feed_forward(LayerNorm(x))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_dim : int\n",
    "        Embedding dimensionality (C).\n",
    "    block_size : int\n",
    "        Maximum sequence length supported (passed to attention heads).\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    Shapes\n",
    "    ------\n",
    "    - Input: (B, T, C)\n",
    "    - Output: (B, T, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, block_size: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(embedding_dim, block_size, n_heads)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run the transformer block on input tensor `x`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, T, C).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, T, C) after attention + feed-forward with residuals.\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MicroGPT(nn.Module):\n",
    "    \"\"\"A small GPT-like model for demonstration and toy datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Number of tokens in the vocabulary (V). Used for the embedding and\n",
    "        the final linear head.\n",
    "    embedding_dim : int\n",
    "        Dimensionality of token and position embeddings (C).\n",
    "    block_size : int\n",
    "        Maximum context length (T). The model uses position embeddings up to\n",
    "        this length and generation is constrained to the last `block_size`\n",
    "        tokens.\n",
    "    n_heads : int\n",
    "        Number of attention heads in each transformer block.\n",
    "    n_layers : int\n",
    "        Number of transformer `Block` layers.\n",
    "\n",
    "    Shapes\n",
    "    ------\n",
    "    - Input: (B, T) token indices\n",
    "    - Output (forward): logits (B, T, V) and (optionally) scalar loss\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This implements a minimal forward pass: token + position embeddings,\n",
    "      stacked transformer blocks, final LayerNorm, and linear head producing\n",
    "      logits for next-token prediction.\n",
    "    - Generation is done autoregressively using the model's learned\n",
    "      distribution over the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, block_size: int, n_heads: int, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx: torch.LongTensor, targets: torch.LongTensor = None):\n",
    "        \"\"\"Compute logits and (optionally) cross-entropy loss for next-token prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.LongTensor, shape (B, T)\n",
    "            Input token indices.\n",
    "        targets : torch.LongTensor, optional, shape (B, T)\n",
    "            Target token indices to compute cross-entropy loss. If `None`, no\n",
    "            loss is returned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor, shape (B, T, V)\n",
    "            Unnormalized log-probabilities for each token in the vocabulary.\n",
    "        loss : torch.Tensor or None\n",
    "            Cross-entropy loss reduced over the batch and sequence dimensions\n",
    "            if `targets` is provided; otherwise `None`.\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        # Token + position embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        # Pass through transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.LongTensor, max_new_tokens: int):\n",
    "        \"\"\"Autoregressively generate new tokens given a starting `idx`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : torch.LongTensor, shape (B, T0)\n",
    "            Conditioning token indices. Only the last `block_size` tokens are\n",
    "            used at each generation step.\n",
    "        max_new_tokens : int\n",
    "            Number of tokens to generate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx : torch.LongTensor\n",
    "            The input tensor concatenated with `max_new_tokens` new token indices.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            # Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def get_batch(data: torch.Tensor, block_size: int, batch_size: int = 16):\n",
    "    \"\"\"Return a random batch of (x, y) pairs for training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torch.Tensor\n",
    "        The full dataset as a 1D tensor of token indices.\n",
    "    block_size : int\n",
    "        Context window size (sequence length).\n",
    "    batch_size : int\n",
    "        Number of sequences to return in the batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.LongTensor, shape (B, T)\n",
    "        Input token indices.\n",
    "    y : torch.LongTensor, shape (B, T)\n",
    "        Target token indices (next-token prediction).\n",
    "    \"\"\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_corpus(path: str, end_token: str = \"<END>\", val_split: float = 0.1):\n",
    "    \"\"\"Load a JSON corpus and prepare it for training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the JSON file containing a list of sentences.\n",
    "    end_token : str\n",
    "        Token to append to each sentence (default: \"<END>\").\n",
    "    val_split : float\n",
    "        Fraction of data to use for validation (default: 0.1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_data : torch.LongTensor\n",
    "        1D tensor of token indices for training.\n",
    "    val_data : torch.LongTensor\n",
    "        1D tensor of token indices for validation.\n",
    "    word2idx : dict\n",
    "        Mapping from words to indices.\n",
    "    idx2word : dict\n",
    "        Mapping from indices to words.\n",
    "    corpus : list\n",
    "        List of sentences with end tokens appended.\n",
    "    vocab_size : int\n",
    "        Number of unique words in the vocabulary.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        corpus = json.load(f)\n",
    "    # Add end token to each sentence\n",
    "    corpus = [s.strip() + \" \" + end_token for s in corpus]\n",
    "    # Build vocabulary (all unique words)\n",
    "    all_text = \" \".join(corpus)\n",
    "    words = list(set(all_text.split()))\n",
    "    vocab_size = len(words)\n",
    "    # Create word <-> index mappings\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    # Convert text to token indices\n",
    "    data = torch.tensor([word2idx[w] for w in all_text.split()], dtype=torch.long)\n",
    "    # Split into train and val\n",
    "    n = int(len(data) * (1 - val_split))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return train_data, val_data, word2idx, idx2word, corpus, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f45ab",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "adebc206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 sentences\n",
      "Vocabulary size: 35\n",
      "Train tokens: 95 | Val tokens: 11\n",
      "Model parameters: 27,747\n",
      "\n",
      "Training...\n",
      "Step   0 | Train: 3.5563 | Val: 3.7721\n",
      "Step  20 | Train: 3.2179 | Val: 3.6243\n",
      "Step  40 | Train: 2.9745 | Val: 3.7129\n",
      "Step  60 | Train: 2.5026 | Val: 3.4522\n",
      "Step  80 | Train: 2.3059 | Val: 3.4478\n",
      "Step 100 | Train: 1.8125 | Val: 3.3178\n",
      "Step 120 | Train: 1.4316 | Val: 3.6147\n",
      "Step 140 | Train: 1.3991 | Val: 3.4502\n",
      "Step 160 | Train: 1.1840 | Val: 3.1982\n",
      "Step 180 | Train: 1.0622 | Val: 3.6726\n",
      "Step 200 | Train: 0.9778 | Val: 3.3387\n",
      "Step 220 | Train: 0.7781 | Val: 3.9346\n",
      "Step 240 | Train: 0.7800 | Val: 3.7933\n",
      "Step 260 | Train: 0.7748 | Val: 4.0200\n",
      "Step 280 | Train: 0.5451 | Val: 4.0197\n",
      "Final val loss: 4.0197\n",
      "\n",
      "==================================================\n",
      "GENERATION\n",
      "==================================================\n",
      "Starting words: 'mary had a little'\n",
      "Generated: mary had a little lamb\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "block_size = 6      # Context window size\n",
    "embedding_dim = 32  # Size of embeddings\n",
    "n_heads = 2         # Number of attention heads\n",
    "n_layers = 2        # Number of transformer blocks\n",
    "lr = 5e-4           # Learning rate\n",
    "epochs = 300        # Training steps\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD AND PREPARE DATA\n",
    "# =============================================================================\n",
    "train_data, val_data, word2idx, idx2word, corpus, vocab_size = load_corpus(\"corpus.json\")\n",
    "print(f\"Loaded {len(corpus)} sentences\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_data)} | Val tokens: {len(val_data)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE MODEL AND OPTIMIZER\n",
    "# =============================================================================\n",
    "model = MicroGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    block_size=block_size,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING LOOP\n",
    "# =============================================================================\n",
    "print(\"\\nTraining...\")\n",
    "for step in range(epochs):\n",
    "    xb, yb = get_batch(train_data, block_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 20 == 0:\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xv, yv = get_batch(val_data, block_size)\n",
    "            _, val_loss = model(xv, yv)\n",
    "        model.train()\n",
    "        print(f\"Step {step:3d} | Train: {loss.item():.4f} | Val: {val_loss.item():.4f}\")\n",
    "print(f\"Final val loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE TEXT\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GENERATION\")\n",
    "print(\"=\" * 50)\n",
    "start_words = corpus[0].split()[:4]\n",
    "start_idx = torch.tensor([[word2idx[w] for w in start_words]], dtype=torch.long)\n",
    "model.eval()\n",
    "output = model.generate(start_idx, max_new_tokens=1)\n",
    "generated = \" \".join(idx2word[int(i)] for i in output[0])\n",
    "print(f\"Starting words: '{' '.join(start_words)}'\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
